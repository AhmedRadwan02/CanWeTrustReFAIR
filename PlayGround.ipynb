{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Embeddings\n",
    "importlib.reload(Embeddings)\n",
    "from Embeddings import Embedders_Five\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "# download fastText\n",
    "fastext_path = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/cc.en.300.bin\"\n",
    "\n",
    "if not os.path.exists(fastext_path):\n",
    "    fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "\n",
    "\n",
    "#Download Word2Vec model\n",
    "word2vec_path = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/word2vec-google-news-300.bin\"\n",
    "\n",
    "if not os.path.exists(word2vec_path):\n",
    "    word2vec_model = api.load('word2vec-google-news-300')\n",
    "    word2vec_model.save_word2vec_format('word2vec-google-news-300.bin', binary=True)\n",
    "\n",
    "# Download GloVe vectors\n",
    "glove_url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "zip_path = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/glove.6B.zip\"\n",
    "glove_txt = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/glove.6B.100d.txt\"\n",
    "glove_word2vec = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/glove.6B.100d.word2vec\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Downloading GloVe vectors...\")\n",
    "    urllib.request.urlretrieve(glove_url, zip_path)\n",
    "    \n",
    "    # Unzip the file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/')\n",
    "    print(\"Download and extraction complete\")\n",
    "    \n",
    "    # Convert to Word2Vec format\n",
    "    print(\"Converting to Word2Vec format...\")\n",
    "    from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "    glove2word2vec(glove_txt, glove_word2vec)\n",
    "    print(\"Conversion complete\")\n",
    "else:\n",
    "    if not os.path.exists(glove_word2vec):\n",
    "        print(\"Converting to Word2Vec format...\")\n",
    "        from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "        glove2word2vec(glove_txt, glove_word2vec)\n",
    "        print(\"Conversion complete\")\n",
    "    print(\"Files already exist\")\n",
    "\n",
    "\n",
    "\n",
    "# Load data\n",
    "user_stories = pd.read_excel(\"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/Dataset/Domain_Classification_Data/Synthetic User Stories.xlsx\")\n",
    "user_stories['Domain'] = user_stories['Domain'].str.lower()\n",
    "\n",
    "# Create embedder instance\n",
    "embedder = Embedders_Five(user_stories[\"User Story\"])\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data_y = label_encoder.fit_transform(user_stories[\"Domain\"])\n",
    "print(\"Number of labels:\", data_y.shape)\n",
    "domains_names = np.unique(user_stories[\"Domain\"])\n",
    "print(\"Unique domains:\", domains_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domains Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# Test FastText\n",
    "print(\"\\n=== FastText Results ===\")\n",
    "fasttext_features = embedder.getFastTextEmbedding()\n",
    "# Split data for FastText\n",
    "X_train_fasttext, X_test_fasttext, y_train_fasttext, y_test_fasttext = train_test_split(\n",
    "    fasttext_features, data_y, test_size=0.2, random_state=42\n",
    ")\n",
    "# LazyClassifier for FastText\n",
    "clf_fasttext = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_fasttext, predictions_fasttext = clf_fasttext.fit(X_train_fasttext, X_test_fasttext, y_train_fasttext, y_test_fasttext)\n",
    "print(\"\\nFastText Models Performance:\")\n",
    "print(models_fasttext)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Test GloVe\n",
    "print(\"\\n=== GloVe Results ===\")\n",
    "glove_features = embedder.getGloVEEmbedding()\n",
    "# Split data for GloVe\n",
    "X_train_glove, X_test_glove, y_train_glove, y_test_glove = train_test_split(\n",
    "    glove_features, data_y, test_size=0.2, random_state=42\n",
    ")\n",
    "# LazyClassifier for GloVe\n",
    "clf_glove = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_glove, predictions_glove = clf_glove.fit(X_train_glove, X_test_glove, y_train_glove, y_test_glove)\n",
    "print(\"\\nGloVe Models Performance:\")\n",
    "print(models_glove)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Test TFIDF\n",
    "print(\"\\n=== TFIDF Results ===\")\n",
    "data_x = embedder.getTFIDFEmbeddings()\n",
    "# Split data for TFIDF\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=42)\n",
    "X_train_dense = X_train\n",
    "X_test_dense = X_test\n",
    "# LazyClassifier for TFIDF\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_tfidf, predictions_tfidf = clf.fit(X_train_dense, X_test_dense, y_train, y_test)\n",
    "print(\"\\nTFIDF Models Performance:\")\n",
    "print(models_tfidf)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Test BERT tokenization approach\n",
    "print(\"\\n=== BERT Results ===\")\n",
    "bert_features = embedder.getBERTEmbeddings()\n",
    "# Convert to float for ML compatibility if needed\n",
    "bert_features = bert_features.astype(np.float32)\n",
    "# Split data for BERT\n",
    "X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(\n",
    "    bert_features, data_y, test_size=0.2, random_state=42\n",
    ")\n",
    "# LazyClassifier for BERT\n",
    "clf_bert = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_bert, predictions_bert = clf_bert.fit(X_train_bert, X_test_bert, y_train_bert, y_test_bert)\n",
    "print(\"\\nBERT Models Performance:\")\n",
    "print(models_bert)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Test Word2Vec\n",
    "print(\"\\n=== Word2Vec Results ===\")\n",
    "w2v_features = embedder.getWord2VecEmbedding()\n",
    "# Split data for Word2Vec\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(\n",
    "    w2v_features, data_y, test_size=0.2, random_state=42\n",
    ")\n",
    "# LazyClassifier for Word2Vec\n",
    "clf_w2v = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_w2v, predictions_w2v = clf_w2v.fit(X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v)\n",
    "print(\"\\nWord2Vec Models Performance:\")\n",
    "print(models_w2v)\n",
    "\n",
    "# Compare best models\n",
    "print(\"\\n=== Performance Comparison ===\")\n",
    "print(\"Best FastText Model:\", models_fasttext.iloc[0])\n",
    "print(\"Best TFIDF Model:\", models_tfidf.iloc[0])\n",
    "print(\"Best BERT Model:\", models_bert.iloc[0])\n",
    "print(\"Best Word2Vec Model:\", models_w2v.iloc[0])\n",
    "print(\"Best GloVe Model:\", models_glove.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitive Features test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'domain': {'name': 'transportation', 'features': ['geography', 'race']},\n",
       " 'tasks': {'classification': ['age',\n",
       "   'geography',\n",
       "   'race',\n",
       "   'sex',\n",
       "   'synthetic',\n",
       "   'birthplace',\n",
       "   'citizenship',\n",
       "   'disability',\n",
       "   'ethnicity',\n",
       "   'family size',\n",
       "   'family wealth',\n",
       "   'gender',\n",
       "   'other sensitive annotations may be present in synsets from the person subtree',\n",
       "   \"people's gender\",\n",
       "   'race (inferred)',\n",
       "   'skin tone',\n",
       "   'skin type',\n",
       "   'textual reference to people and their demographics',\n",
       "   'textual references to people and their demographics',\n",
       "   'race/ethnicity',\n",
       "   'religion',\n",
       "   'sexual orientation'],\n",
       "  'regression': ['age',\n",
       "   'ethnicity',\n",
       "   'financial status',\n",
       "   'gender',\n",
       "   'geography',\n",
       "   'race',\n",
       "   'sex'],\n",
       "  'ranking': ['activity',\n",
       "   'age',\n",
       "   'and gender',\n",
       "   'birth category',\n",
       "   'ethnic group',\n",
       "   'gender',\n",
       "   'geography',\n",
       "   'news provider',\n",
       "   'ownership',\n",
       "   'race',\n",
       "   'sex',\n",
       "   'tour availability'],\n",
       "  'representation learning': ['age',\n",
       "   'and gender',\n",
       "   'ethnicity',\n",
       "   'financial status',\n",
       "   'gender',\n",
       "   'geography',\n",
       "   'race',\n",
       "   'race (inferrred)',\n",
       "   'sex',\n",
       "   'skin color',\n",
       "   'skin tone'],\n",
       "  'clustering': ['age',\n",
       "   'author',\n",
       "   'ethnicity',\n",
       "   'gender',\n",
       "   'geography',\n",
       "   'race',\n",
       "   'sex',\n",
       "   'textual references to people and their demographics'],\n",
       "  'anomaly detection': ['age', 'gender', 'race', 'sex', 'skin tone'],\n",
       "  'districting': ['political affiliation (representation in different precincts)',\n",
       "   'race'],\n",
       "  'spatio-temporal process learning': ['age',\n",
       "   'and gender (of victim)',\n",
       "   'geography',\n",
       "   'race'],\n",
       "  'graph diffusion': ['age',\n",
       "   'ethnicity',\n",
       "   'gender',\n",
       "   'geography',\n",
       "   'race and ethnicity',\n",
       "   'sexual orientation'],\n",
       "  'graph augmentation': ['gender', 'race'],\n",
       "  'resource allocation': ['synthetic', 'age', 'race', 'sex', 'geography'],\n",
       "  'data summarization': ['age',\n",
       "   'caste',\n",
       "   'gender',\n",
       "   'geography',\n",
       "   'male/female',\n",
       "   'race',\n",
       "   'sex',\n",
       "   'skin tone'],\n",
       "  'graph mining': ['age',\n",
       "   'author',\n",
       "   'demographics of people featured in entities and their relations',\n",
       "   'gender',\n",
       "   'geography'],\n",
       "  'entity resolution': ['business size', 'geography'],\n",
       "  'sentiment analysis': ['race', 'sex'],\n",
       "  'bias detection in word embeddings': ['race',\n",
       "   'sex',\n",
       "   'gender',\n",
       "   'textual references to people and their demographics'],\n",
       "  'bias detection in language models': ['gender',\n",
       "   'political leaning',\n",
       "   'profession',\n",
       "   'race',\n",
       "   'religion',\n",
       "   'textual references to people and their demographics',\n",
       "   'age',\n",
       "   'artist',\n",
       "   'ethnicity',\n",
       "   'geography',\n",
       "   'other sensitive annotations may be present in synsets from the person subtree',\n",
       "   \"people's gender\",\n",
       "   'record label',\n",
       "   'sex',\n",
       "   'skin color',\n",
       "   'skin type'],\n",
       "  'machine translation': ['gender'],\n",
       "  'speech recognition': ['dialect', 'gender', 'geography']}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from SensitiveFeaturesMapping import SensitiveFeaturesMapper\n",
    "import importlib\n",
    "import Embeddings\n",
    "importlib.reload(Embeddings)\n",
    "from Embeddings import Embedders_Five\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "import gensim.downloader as api\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "\n",
    "# Load data\n",
    "user_stories = pd.read_excel(\"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/Dataset/Domain_Classification_Data/Synthetic User Stories.xlsx\")\n",
    "user_stories['Domain'] = user_stories['Domain'].str.lower()\n",
    "ontology = SensitiveFeaturesMapper(\"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/Dataset/Feature_Extraction/domains-features-mapping.csv\",\n",
    "                                   \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/Dataset/Feature_Extraction/tasks-features-mapping.csv\")\n",
    "\n",
    "\n",
    "labels = pd.read_excel(\"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/Dataset/ML_Tasks_Classification_Data/Keyword labelled.xlsx\", header=None)\n",
    "labels[2] = labels[2].apply(lambda x: x.lower())\n",
    "categories_column = []\n",
    "for row in labels.iterrows():\n",
    "    current_labels = []\n",
    "    for label in row[1][3:]:\n",
    "        if isinstance(label, str):\n",
    "            current_labels.append(label.lower())\n",
    "    categories_column.append(current_labels)\n",
    "labels[\"Categories array\"] = categories_column\n",
    "labels[[2, \"Categories array\"]]\n",
    "\n",
    "target = []\n",
    "counter = 0\n",
    "for row in user_stories.iterrows():\n",
    "    target.append(labels[labels[2]==row[1][\"Machine Learning Task\"].lower()][\"Categories array\"].values[0])\n",
    "    counter += 1\n",
    "user_stories[\"Target\"] = target\n",
    "user_stories[[\"User Story\",\"Target\"]]\n",
    "\n",
    "ontology.get_sensitive_features(user_stories[\"Target\"][10000],user_stories[\"Domain\"][10000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
