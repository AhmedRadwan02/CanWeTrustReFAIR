{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Embeddings\n",
    "importlib.reload(Embeddings)\n",
    "from Embeddings import Embedders_Five\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "import gensim.downloader as api\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "# download fastText\n",
    "fastext_path = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/cc.en.300.bin\"\n",
    "\n",
    "if not os.path.exists(fastext_path):\n",
    "    fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "\n",
    "\n",
    "#Download Word2Vec model\n",
    "word2vec_path = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/word2vec-google-news-300.bin\"\n",
    "\n",
    "if not os.path.exists(word2vec_path):\n",
    "    word2vec_model = api.load('word2vec-google-news-300')\n",
    "    word2vec_model.save_word2vec_format('word2vec-google-news-300.bin', binary=True)\n",
    "\n",
    "# Download GloVe vectors\n",
    "glove_url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "zip_path = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/glove.6B.zip\"\n",
    "glove_txt = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/glove.6B.100d.txt\"\n",
    "glove_word2vec = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/glove.6B.100d.word2vec\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Downloading GloVe vectors...\")\n",
    "    urllib.request.urlretrieve(glove_url, zip_path)\n",
    "    \n",
    "    # Unzip the file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/')\n",
    "    print(\"Download and extraction complete\")\n",
    "    \n",
    "    # Convert to Word2Vec format\n",
    "    print(\"Converting to Word2Vec format...\")\n",
    "    from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "    glove2word2vec(glove_txt, glove_word2vec)\n",
    "    print(\"Conversion complete\")\n",
    "else:\n",
    "    if not os.path.exists(glove_word2vec):\n",
    "        print(\"Converting to Word2Vec format...\")\n",
    "        from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "        glove2word2vec(glove_txt, glove_word2vec)\n",
    "        print(\"Conversion complete\")\n",
    "    print(\"Files already exist\")\n",
    "\n",
    "\n",
    "\n",
    "# Load data\n",
    "user_stories = pd.read_excel(\"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/Dataset/Domain_Classification_Data/Synthetic User Stories.xlsx\")\n",
    "user_stories['Domain'] = user_stories['Domain'].str.lower()\n",
    "\n",
    "# Create embedder instance\n",
    "embedder = Embedders_Five(user_stories[\"User Story\"])\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data_y = label_encoder.fit_transform(user_stories[\"Domain\"])\n",
    "print(\"Number of labels:\", data_y.shape)\n",
    "domains_names = np.unique(user_stories[\"Domain\"])\n",
    "print(\"Unique domains:\", domains_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# Test FastText\n",
    "print(\"\\n=== FastText Results ===\")\n",
    "fasttext_features = embedder.getFastTextEmbedding()\n",
    "# Split data for FastText\n",
    "X_train_fasttext, X_test_fasttext, y_train_fasttext, y_test_fasttext = train_test_split(\n",
    "    fasttext_features, data_y, test_size=0.2, random_state=42\n",
    ")\n",
    "# LazyClassifier for FastText\n",
    "clf_fasttext = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_fasttext, predictions_fasttext = clf_fasttext.fit(X_train_fasttext, X_test_fasttext, y_train_fasttext, y_test_fasttext)\n",
    "print(\"\\nFastText Models Performance:\")\n",
    "print(models_fasttext)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Test GloVe\n",
    "print(\"\\n=== GloVe Results ===\")\n",
    "glove_features = embedder.getGloVEEmbedding()\n",
    "# Split data for GloVe\n",
    "X_train_glove, X_test_glove, y_train_glove, y_test_glove = train_test_split(\n",
    "    glove_features, data_y, test_size=0.2, random_state=42\n",
    ")\n",
    "# LazyClassifier for GloVe\n",
    "clf_glove = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_glove, predictions_glove = clf_glove.fit(X_train_glove, X_test_glove, y_train_glove, y_test_glove)\n",
    "print(\"\\nGloVe Models Performance:\")\n",
    "print(models_glove)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Test TFIDF\n",
    "print(\"\\n=== TFIDF Results ===\")\n",
    "data_x = embedder.getTFIDFEmbeddings()\n",
    "# Split data for TFIDF\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=42)\n",
    "X_train_dense = X_train\n",
    "X_test_dense = X_test\n",
    "# LazyClassifier for TFIDF\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_tfidf, predictions_tfidf = clf.fit(X_train_dense, X_test_dense, y_train, y_test)\n",
    "print(\"\\nTFIDF Models Performance:\")\n",
    "print(models_tfidf)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Test BERT tokenization approach\n",
    "print(\"\\n=== BERT Results ===\")\n",
    "bert_features = embedder.getBERTEmbeddings()\n",
    "# Convert to float for ML compatibility if needed\n",
    "bert_features = bert_features.astype(np.float32)\n",
    "# Split data for BERT\n",
    "X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(\n",
    "    bert_features, data_y, test_size=0.2, random_state=42\n",
    ")\n",
    "# LazyClassifier for BERT\n",
    "clf_bert = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_bert, predictions_bert = clf_bert.fit(X_train_bert, X_test_bert, y_train_bert, y_test_bert)\n",
    "print(\"\\nBERT Models Performance:\")\n",
    "print(models_bert)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Test Word2Vec\n",
    "print(\"\\n=== Word2Vec Results ===\")\n",
    "w2v_features = embedder.getWord2VecEmbedding()\n",
    "# Split data for Word2Vec\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(\n",
    "    w2v_features, data_y, test_size=0.2, random_state=42\n",
    ")\n",
    "# LazyClassifier for Word2Vec\n",
    "clf_w2v = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_w2v, predictions_w2v = clf_w2v.fit(X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v)\n",
    "print(\"\\nWord2Vec Models Performance:\")\n",
    "print(models_w2v)\n",
    "\n",
    "# Compare best models\n",
    "print(\"\\n=== Performance Comparison ===\")\n",
    "print(\"Best FastText Model:\", models_fasttext.iloc[0])\n",
    "print(\"Best TFIDF Model:\", models_tfidf.iloc[0])\n",
    "print(\"Best BERT Model:\", models_bert.iloc[0])\n",
    "print(\"Best Word2Vec Model:\", models_w2v.iloc[0])\n",
    "print(\"Best GloVe Model:\", models_glove.iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
