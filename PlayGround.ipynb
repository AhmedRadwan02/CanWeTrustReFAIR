{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure to change all paths as they are all local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Embeddings\n",
    "importlib.reload(Embeddings)\n",
    "from Embeddings import Embedders_Five\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "# download fastText\n",
    "fastext_path = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/cc.en.300.bin\"\n",
    "\n",
    "if not os.path.exists(fastext_path):\n",
    "    fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "\n",
    "\n",
    "#Download Word2Vec model\n",
    "word2vec_path = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/word2vec-google-news-300.bin\"\n",
    "\n",
    "if not os.path.exists(word2vec_path):\n",
    "    word2vec_model = api.load('word2vec-google-news-300')\n",
    "    word2vec_model.save_word2vec_format('word2vec-google-news-300.bin', binary=True)\n",
    "\n",
    "# Download GloVe vectors\n",
    "glove_url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "zip_path = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/glove.6B.zip\"\n",
    "glove_txt = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/glove.6B.100d.txt\"\n",
    "glove_word2vec = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/glove.6B.100d.word2vec\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Downloading GloVe vectors...\")\n",
    "    urllib.request.urlretrieve(glove_url, zip_path)\n",
    "    \n",
    "    # Unzip the file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/')\n",
    "    print(\"Download and extraction complete\")\n",
    "    \n",
    "    # Convert to Word2Vec format\n",
    "    print(\"Converting to Word2Vec format...\")\n",
    "    from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "    glove2word2vec(glove_txt, glove_word2vec)\n",
    "    print(\"Conversion complete\")\n",
    "else:\n",
    "    if not os.path.exists(glove_word2vec):\n",
    "        print(\"Converting to Word2Vec format...\")\n",
    "        from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "        glove2word2vec(glove_txt, glove_word2vec)\n",
    "        print(\"Conversion complete\")\n",
    "    print(\"Files already exist\")\n",
    "\n",
    "\n",
    "\n",
    "# Load data\n",
    "user_stories = pd.read_excel(\"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/Dataset/Domain_Classification_Data/Synthetic User Stories.xlsx\")\n",
    "user_stories['Domain'] = user_stories['Domain'].str.lower()\n",
    "\n",
    "# Create embedder instance\n",
    "embedder = Embedders_Five(user_stories[\"User Story\"])\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data_y = label_encoder.fit_transform(user_stories[\"Domain\"])\n",
    "print(\"Number of labels:\", data_y.shape)\n",
    "domains_names = np.unique(user_stories[\"Domain\"])\n",
    "print(\"Unique domains:\", domains_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domains Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# Test FastText\n",
    "print(\"\\n=== FastText Results ===\")\n",
    "fasttext_features = embedder.getFastTextEmbedding()\n",
    "# Split data for FastText\n",
    "X_train_fasttext, X_test_fasttext, y_train_fasttext, y_test_fasttext = train_test_split(\n",
    "    fasttext_features, data_y, test_size=0.2, random_state=42\n",
    ")\n",
    "# LazyClassifier for FastText\n",
    "clf_fasttext = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_fasttext, predictions_fasttext = clf_fasttext.fit(X_train_fasttext, X_test_fasttext, y_train_fasttext, y_test_fasttext)\n",
    "print(\"\\nFastText Models Performance:\")\n",
    "print(models_fasttext)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Test GloVe\n",
    "print(\"\\n=== GloVe Results ===\")\n",
    "glove_features = embedder.getGloVEEmbedding()\n",
    "# Split data for GloVe\n",
    "X_train_glove, X_test_glove, y_train_glove, y_test_glove = train_test_split(\n",
    "    glove_features, data_y, test_size=0.2, random_state=42\n",
    ")\n",
    "# LazyClassifier for GloVe\n",
    "clf_glove = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_glove, predictions_glove = clf_glove.fit(X_train_glove, X_test_glove, y_train_glove, y_test_glove)\n",
    "print(\"\\nGloVe Models Performance:\")\n",
    "print(models_glove)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Test TFIDF\n",
    "print(\"\\n=== TFIDF Results ===\")\n",
    "data_x = embedder.getTFIDFEmbeddings()\n",
    "# Split data for TFIDF\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=42)\n",
    "X_train_dense = X_train\n",
    "X_test_dense = X_test\n",
    "# LazyClassifier for TFIDF\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_tfidf, predictions_tfidf = clf.fit(X_train_dense, X_test_dense, y_train, y_test)\n",
    "print(\"\\nTFIDF Models Performance:\")\n",
    "print(models_tfidf)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Test BERT tokenization approach\n",
    "print(\"\\n=== BERT Results ===\")\n",
    "bert_features = embedder.getBERTEmbeddings()\n",
    "# Convert to float for ML compatibility if needed\n",
    "bert_features = bert_features.astype(np.float32)\n",
    "# Split data for BERT\n",
    "X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(\n",
    "    bert_features, data_y, test_size=0.2, random_state=42\n",
    ")\n",
    "# LazyClassifier for BERT\n",
    "clf_bert = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_bert, predictions_bert = clf_bert.fit(X_train_bert, X_test_bert, y_train_bert, y_test_bert)\n",
    "print(\"\\nBERT Models Performance:\")\n",
    "print(models_bert)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Test Word2Vec\n",
    "print(\"\\n=== Word2Vec Results ===\")\n",
    "w2v_features = embedder.getWord2VecEmbedding()\n",
    "# Split data for Word2Vec\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(\n",
    "    w2v_features, data_y, test_size=0.2, random_state=42\n",
    ")\n",
    "# LazyClassifier for Word2Vec\n",
    "clf_w2v = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_w2v, predictions_w2v = clf_w2v.fit(X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v)\n",
    "print(\"\\nWord2Vec Models Performance:\")\n",
    "print(models_w2v)\n",
    "\n",
    "# Compare best models\n",
    "print(\"\\n=== Performance Comparison ===\")\n",
    "print(\"Best FastText Model:\", models_fasttext.iloc[0])\n",
    "print(\"Best TFIDF Model:\", models_tfidf.iloc[0])\n",
    "print(\"Best BERT Model:\", models_bert.iloc[0])\n",
    "print(\"Best Word2Vec Model:\", models_w2v.iloc[0])\n",
    "print(\"Best GloVe Model:\", models_glove.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitive Features test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              User Story  \\\n",
      "0      A group of researchers is using abstractive su...   \n",
      "1      As a plant scientist, I want to use abstractiv...   \n",
      "2      As a molecular biologist, I want to use action...   \n",
      "3      As a plant scientist, I want to use action mod...   \n",
      "4      As a bioinformatics researcher, I want to use ...   \n",
      "...                                                  ...   \n",
      "12396  As a computer vision researcher, I want to use...   \n",
      "12397  As a network engineer, I want to use word2vec ...   \n",
      "12398  As a computer vision researcher, I want to use...   \n",
      "12399  As a network engineer, I want to use WordNet t...   \n",
      "12400  As a computer vision researcher, I want to use...   \n",
      "\n",
      "                                                  Target  \n",
      "0                                   [data summarization]  \n",
      "1                                   [data summarization]  \n",
      "2                                                [other]  \n",
      "3                                                [other]  \n",
      "4      [representation learning, classification, regr...  \n",
      "...                                                  ...  \n",
      "12396  [classification, ranking, matching, clustering...  \n",
      "12397  [classification, ranking, matching, clustering...  \n",
      "12398  [classification, ranking, matching, clustering...  \n",
      "12399  [representation learning, clustering, matching...  \n",
      "12400  [representation learning, clustering, matching...  \n",
      "\n",
      "[12401 rows x 2 columns]\n",
      "Shape of original binary labels: (12401, 26)\n",
      "Shape of transformed labels: (12401,)\n",
      "227\n"
     ]
    }
   ],
   "source": [
    "from SensitiveFeaturesMapping import SensitiveFeaturesMapper\n",
    "import importlib\n",
    "import Embeddings\n",
    "importlib.reload(Embeddings)\n",
    "from Embeddings import Embedders_Five\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "import gensim.downloader as api\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "\n",
    "# Load data\n",
    "user_stories = pd.read_excel(\"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/Dataset/Domain_Classification_Data/Synthetic User Stories.xlsx\")\n",
    "user_stories['Domain'] = user_stories['Domain'].str.lower()\n",
    "ontology = SensitiveFeaturesMapper(\"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/Dataset/Feature_Extraction/domains-features-mapping.csv\",\n",
    "                                   \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/Dataset/Feature_Extraction/tasks-features-mapping.csv\")\n",
    "\n",
    "\n",
    "labels = pd.read_excel(\"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/Dataset/ML_Tasks_Classification_Data/Keyword labelled.xlsx\", header=None)\n",
    "labels[2] = labels[2].apply(lambda x: x.lower())\n",
    "categories_column = []\n",
    "for row in labels.iterrows():\n",
    "    current_labels = []\n",
    "    for label in row[1][3:]:\n",
    "        if isinstance(label, str):\n",
    "            current_labels.append(label.lower())\n",
    "    categories_column.append(current_labels)\n",
    "labels[\"Categories array\"] = categories_column\n",
    "labels[[2, \"Categories array\"]]\n",
    "\n",
    "target = []\n",
    "counter = 0\n",
    "for row in user_stories.iterrows():\n",
    "    target.append(labels[labels[2]==row[1][\"Machine Learning Task\"].lower()][\"Categories array\"].values[0])\n",
    "    counter += 1\n",
    "user_stories[\"Target\"] = target\n",
    "user_stories[[\"User Story\",\"Target\"]]\n",
    "\n",
    "ontology.get_sensitive_features(user_stories[\"Target\"][10000],user_stories[\"Domain\"][10000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "# Reload custom modules\n",
    "import Embeddings\n",
    "import ML_Classification\n",
    "importlib.reload(Embeddings)\n",
    "importlib.reload(ML_Classification)\n",
    "from Embeddings import Embedders_Five\n",
    "from ML_Classification import ML_Classification\n",
    "\n",
    "# Load and preprocess data\n",
    "# Make sure to change the path, copy path of dataset -> domain_classification_data\n",
    "user_stories = pd.read_excel(\"/Users/Claudia/Projects/CanWeTrustReFAIR_/CanWeTrustReFAIR/Dataset/Domain_Classification_Data/Synthetic User Stories.xlsx\")\n",
    "user_stories['Domain'] = user_stories['Domain'].str.lower()\n",
    "\n",
    "# Create embedder instance\n",
    "embedder = Embedders_Five(user_stories[\"User Story\"])\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data_y = label_encoder.fit_transform(user_stories[\"Domain\"])\n",
    "\n",
    "# Print dataset information\n",
    "print(\"Number of labels:\", data_y.shape)\n",
    "domains_names = np.unique(user_stories[\"Domain\"])\n",
    "print(\"Unique domains:\", domains_names)\n",
    "\n",
    "# Generate TFIDF embeddings\n",
    "print(\"\\n=== TFIDF Results ===\")\n",
    "data_x = embedder.getTFIDFEmbeddings()\n",
    "\n",
    "# Split data for TFIDF\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_x, data_y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "classifier = ML_Classification()\n",
    "best_model, performance_df = classifier.train_ml_models(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"\\nAll Models Performance:\") \n",
    "print(performance_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
