{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Make sure to change all paths as they are all local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Embeddings\n",
    "importlib.reload(Embeddings)\n",
    "from Embeddings import Embedders_Five\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "# download fastText\n",
    "fastext_path = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/cc.en.300.bin\"\n",
    "\n",
    "if not os.path.exists(fastext_path):\n",
    "    fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "\n",
    "\n",
    "#Download Word2Vec model\n",
    "word2vec_path = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/word2vec-google-news-300.bin\"\n",
    "\n",
    "if not os.path.exists(word2vec_path):\n",
    "    word2vec_model = api.load('word2vec-google-news-300')\n",
    "    word2vec_model.save_word2vec_format('word2vec-google-news-300.bin', binary=True)\n",
    "\n",
    "# Download GloVe vectors\n",
    "glove_url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "zip_path = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/glove.6B.zip\"\n",
    "glove_txt = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/glove.6B.100d.txt\"\n",
    "glove_word2vec = \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/glove.6B.100d.word2vec\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Downloading GloVe vectors...\")\n",
    "    urllib.request.urlretrieve(glove_url, zip_path)\n",
    "    \n",
    "    # Unzip the file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/')\n",
    "    print(\"Download and extraction complete\")\n",
    "    \n",
    "    # Convert to Word2Vec format\n",
    "    print(\"Converting to Word2Vec format...\")\n",
    "    from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "    glove2word2vec(glove_txt, glove_word2vec)\n",
    "    print(\"Conversion complete\")\n",
    "else:\n",
    "    if not os.path.exists(glove_word2vec):\n",
    "        print(\"Converting to Word2Vec format...\")\n",
    "        from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "        glove2word2vec(glove_txt, glove_word2vec)\n",
    "        print(\"Conversion complete\")\n",
    "    print(\"Files already exist\")\n",
    "\n",
    "\n",
    "\n",
    "# Load data\n",
    "user_stories = pd.read_excel(\"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/Dataset/Domain_Classification_Data/Synthetic User Stories.xlsx\")\n",
    "user_stories['Domain'] = user_stories['Domain'].str.lower()\n",
    "\n",
    "# Create embedder instance\n",
    "embedder = Embedders_Five(user_stories[\"User Story\"])\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data_y = label_encoder.fit_transform(user_stories[\"Domain\"])\n",
    "print(\"Number of labels:\", data_y.shape)\n",
    "domains_names = np.unique(user_stories[\"Domain\"])\n",
    "print(\"Unique domains:\", domains_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Domains Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# Test FastText\n",
    "print(\"\\n=== FastText Results ===\")\n",
    "fasttext_features = embedder.getFastTextEmbedding()\n",
    "# Split data for FastText\n",
    "X_train_fasttext, X_test_fasttext, y_train_fasttext, y_test_fasttext = train_test_split(\n",
    "    fasttext_features, data_y, test_size=0.2, random_state=42\n",
    ")\n",
    "# LazyClassifier for FastText\n",
    "clf_fasttext = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_fasttext, predictions_fasttext = clf_fasttext.fit(X_train_fasttext, X_test_fasttext, y_train_fasttext, y_test_fasttext)\n",
    "print(\"\\nFastText Models Performance:\")\n",
    "print(models_fasttext)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Test GloVe\n",
    "print(\"\\n=== GloVe Results ===\")\n",
    "glove_features = embedder.getGloVEEmbedding()\n",
    "# Split data for GloVe\n",
    "X_train_glove, X_test_glove, y_train_glove, y_test_glove = train_test_split(\n",
    "    glove_features, data_y, test_size=0.2, random_state=42\n",
    ")\n",
    "# LazyClassifier for GloVe\n",
    "clf_glove = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_glove, predictions_glove = clf_glove.fit(X_train_glove, X_test_glove, y_train_glove, y_test_glove)\n",
    "print(\"\\nGloVe Models Performance:\")\n",
    "print(models_glove)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Test TFIDF\n",
    "print(\"\\n=== TFIDF Results ===\")\n",
    "data_x = embedder.getTFIDFEmbeddings()\n",
    "# Split data for TFIDF\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=42)\n",
    "X_train_dense = X_train\n",
    "X_test_dense = X_test\n",
    "# LazyClassifier for TFIDF\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_tfidf, predictions_tfidf = clf.fit(X_train_dense, X_test_dense, y_train, y_test)\n",
    "print(\"\\nTFIDF Models Performance:\")\n",
    "print(models_tfidf)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Test BERT tokenization approach\n",
    "print(\"\\n=== BERT Results ===\")\n",
    "bert_features = embedder.getBERTEmbeddings()\n",
    "# Convert to float for ML compatibility if needed\n",
    "bert_features = bert_features.astype(np.float32)\n",
    "# Split data for BERT\n",
    "X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(\n",
    "    bert_features, data_y, test_size=0.2, random_state=42\n",
    ")\n",
    "# LazyClassifier for BERT\n",
    "clf_bert = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_bert, predictions_bert = clf_bert.fit(X_train_bert, X_test_bert, y_train_bert, y_test_bert)\n",
    "print(\"\\nBERT Models Performance:\")\n",
    "print(models_bert)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# Test Word2Vec\n",
    "print(\"\\n=== Word2Vec Results ===\")\n",
    "w2v_features = embedder.getWord2VecEmbedding()\n",
    "# Split data for Word2Vec\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(\n",
    "    w2v_features, data_y, test_size=0.2, random_state=42\n",
    ")\n",
    "# LazyClassifier for Word2Vec\n",
    "clf_w2v = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_w2v, predictions_w2v = clf_w2v.fit(X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v)\n",
    "print(\"\\nWord2Vec Models Performance:\")\n",
    "print(models_w2v)\n",
    "\n",
    "# Compare best models\n",
    "print(\"\\n=== Performance Comparison ===\")\n",
    "print(\"Best FastText Model:\", models_fasttext.iloc[0])\n",
    "print(\"Best TFIDF Model:\", models_tfidf.iloc[0])\n",
    "print(\"Best BERT Model:\", models_bert.iloc[0])\n",
    "print(\"Best Word2Vec Model:\", models_w2v.iloc[0])\n",
    "print(\"Best GloVe Model:\", models_glove.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sensitive Features test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              User Story  \\\n",
      "0      A group of researchers is using abstractive su...   \n",
      "1      As a plant scientist, I want to use abstractiv...   \n",
      "2      As a molecular biologist, I want to use action...   \n",
      "3      As a plant scientist, I want to use action mod...   \n",
      "4      As a bioinformatics researcher, I want to use ...   \n",
      "...                                                  ...   \n",
      "12396  As a computer vision researcher, I want to use...   \n",
      "12397  As a network engineer, I want to use word2vec ...   \n",
      "12398  As a computer vision researcher, I want to use...   \n",
      "12399  As a network engineer, I want to use WordNet t...   \n",
      "12400  As a computer vision researcher, I want to use...   \n",
      "\n",
      "                                                  Target  \n",
      "0                                   [data summarization]  \n",
      "1                                   [data summarization]  \n",
      "2                                                [other]  \n",
      "3                                                [other]  \n",
      "4      [representation learning, classification, regr...  \n",
      "...                                                  ...  \n",
      "12396  [classification, ranking, matching, clustering...  \n",
      "12397  [classification, ranking, matching, clustering...  \n",
      "12398  [classification, ranking, matching, clustering...  \n",
      "12399  [representation learning, clustering, matching...  \n",
      "12400  [representation learning, clustering, matching...  \n",
      "\n",
      "[12401 rows x 2 columns]\n",
      "Shape of original binary labels: (12401, 26)\n",
      "Shape of transformed labels: (12401,)\n",
      "227\n"
     ]
    }
   ],
   "source": [
    "from SensitiveFeaturesMapping import SensitiveFeaturesMapper\n",
    "import importlib\n",
    "import Embeddings\n",
    "importlib.reload(Embeddings)\n",
    "from Embeddings import Embedders_Five\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "import gensim.downloader as api\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "\n",
    "# Load data\n",
    "user_stories = pd.read_excel(\"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/Dataset/Domain_Classification_Data/Synthetic User Stories.xlsx\")\n",
    "user_stories['Domain'] = user_stories['Domain'].str.lower()\n",
    "ontology = SensitiveFeaturesMapper(\"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/Dataset/Feature_Extraction/domains-features-mapping.csv\",\n",
    "                                   \"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/Dataset/Feature_Extraction/tasks-features-mapping.csv\")\n",
    "\n",
    "\n",
    "labels = pd.read_excel(\"/Users/ahmed/Desktop/CanWeTrustReFAIR/CanWeTrustReFAIR/Dataset/ML_Tasks_Classification_Data/Keyword labelled.xlsx\", header=None)\n",
    "labels[2] = labels[2].apply(lambda x: x.lower())\n",
    "categories_column = []\n",
    "for row in labels.iterrows():\n",
    "    current_labels = []\n",
    "    for label in row[1][3:]:\n",
    "        if isinstance(label, str):\n",
    "            current_labels.append(label.lower())\n",
    "    categories_column.append(current_labels)\n",
    "labels[\"Categories array\"] = categories_column\n",
    "labels[[2, \"Categories array\"]]\n",
    "\n",
    "target = []\n",
    "counter = 0\n",
    "for row in user_stories.iterrows():\n",
    "    target.append(labels[labels[2]==row[1][\"Machine Learning Task\"].lower()][\"Categories array\"].values[0])\n",
    "    counter += 1\n",
    "user_stories[\"Target\"] = target\n",
    "user_stories[[\"User Story\",\"Target\"]]\n",
    "\n",
    "ontology.get_sensitive_features(user_stories[\"Target\"][10000],user_stories[\"Domain\"][10000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "# Reload custom modules\n",
    "import Embeddings\n",
    "import ML_Classification\n",
    "importlib.reload(Embeddings)\n",
    "importlib.reload(ML_Classification)\n",
    "from Embeddings import Embedders_Five\n",
    "from ML_Classification import ML_Classification\n",
    "\n",
    "# Load and preprocess data\n",
    "# Make sure to change the path, copy path of dataset -> domain_classification_data\n",
    "user_stories = pd.read_excel(\"/Users/Claudia/Projects/CanWeTrustReFAIR_/CanWeTrustReFAIR/Dataset/Domain_Classification_Data/Synthetic User Stories.xlsx\")\n",
    "user_stories['Domain'] = user_stories['Domain'].str.lower()\n",
    "\n",
    "# Create embedder instance\n",
    "embedder = Embedders_Five(user_stories[\"User Story\"])\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data_y = label_encoder.fit_transform(user_stories[\"Domain\"])\n",
    "\n",
    "# Print dataset information\n",
    "print(\"Number of labels:\", data_y.shape)\n",
    "domains_names = np.unique(user_stories[\"Domain\"])\n",
    "print(\"Unique domains:\", domains_names)\n",
    "\n",
    "# Generate TFIDF embeddings\n",
    "print(\"\\n=== TFIDF Results ===\")\n",
    "data_x = embedder.getTFIDFEmbeddings()\n",
    "\n",
    "# Split data for TFIDF\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_x, data_y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "classifier = ML_Classification()\n",
    "best_model, performance_df = classifier.train_ml_models(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"\\nAll Models Performance:\") \n",
    "print(performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MoJo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Story</th>\n",
       "      <th>Oracle Domain</th>\n",
       "      <th>Oracle Tasks &amp; Sensitive Features</th>\n",
       "      <th>Oracle Unique Sensitive Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A bioinformatics company is using statistical ...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{'classification': [], 'graph mining': []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A bioinformatics company is using text generat...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{'graph mining': []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A bioinformatics company is using visual quest...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{'classification': []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A bioinformatics researcher is developing a sy...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{'graph mining': []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A bioinformatics researcher is interested in i...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A bioinformatics researcher is using text to s...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A bioinformatics researcher is working on a pr...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A bioinformatics team is using hierarchical cl...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{'graph mining': []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A bioinformatics team is using learning under ...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{'classification': []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A bioinformatics team is using medoids to iden...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A biologist is analyzing large amounts of scie...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{'classification': [], 'graph mining': []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A biologist is searching a database for releva...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{'graph mining': []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A biologist wants to extract gene and protein ...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A biotech company is using natural language un...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{'classification': []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A cardiologist is using machine learning to cl...</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           User Story Oracle Domain  \\\n",
       "0   A bioinformatics company is using statistical ...       Biology   \n",
       "1   A bioinformatics company is using text generat...       Biology   \n",
       "2   A bioinformatics company is using visual quest...       Biology   \n",
       "3   A bioinformatics researcher is developing a sy...       Biology   \n",
       "4   A bioinformatics researcher is interested in i...       Biology   \n",
       "5   A bioinformatics researcher is using text to s...       Biology   \n",
       "6   A bioinformatics researcher is working on a pr...       Biology   \n",
       "7   A bioinformatics team is using hierarchical cl...       Biology   \n",
       "8   A bioinformatics team is using learning under ...       Biology   \n",
       "9   A bioinformatics team is using medoids to iden...       Biology   \n",
       "10  A biologist is analyzing large amounts of scie...       Biology   \n",
       "11  A biologist is searching a database for releva...       Biology   \n",
       "12  A biologist wants to extract gene and protein ...       Biology   \n",
       "13  A biotech company is using natural language un...       Biology   \n",
       "14  A cardiologist is using machine learning to cl...    Cardiology   \n",
       "\n",
       "             Oracle Tasks & Sensitive Features  \\\n",
       "0   {'classification': [], 'graph mining': []}   \n",
       "1                         {'graph mining': []}   \n",
       "2                       {'classification': []}   \n",
       "3                         {'graph mining': []}   \n",
       "4                                           {}   \n",
       "5                                           {}   \n",
       "6                                           {}   \n",
       "7                         {'graph mining': []}   \n",
       "8                       {'classification': []}   \n",
       "9                                           {}   \n",
       "10  {'classification': [], 'graph mining': []}   \n",
       "11                        {'graph mining': []}   \n",
       "12                                          {}   \n",
       "13                      {'classification': []}   \n",
       "14                                          {}   \n",
       "\n",
       "   Oracle Unique Sensitive Features  \n",
       "0                                {}  \n",
       "1                                {}  \n",
       "2                                {}  \n",
       "3                                {}  \n",
       "4                                {}  \n",
       "5                                {}  \n",
       "6                                {}  \n",
       "7                                {}  \n",
       "8                                {}  \n",
       "9                                {}  \n",
       "10                               {}  \n",
       "11                               {}  \n",
       "12                               {}  \n",
       "13                               {}  \n",
       "14                               {}  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import pickle\n",
    "from Embeddings import Embedders_Five\n",
    "data = pd.read_excel(\"./Dataset/SensitiveFeaturesValidation_Data/oracle-predictions.xlsx\")\n",
    "domain_task_mapping = pd.read_csv(\"./Dataset/domains-tasks-mapping.csv\")\n",
    "domains_mapping = pd.read_csv(\"./Dataset/Feature_Extraction/domains-features-mapping.csv\")\n",
    "tasks_mapping = pd.read_csv(\"./Dataset/Feature_Extraction/tasks-features-mapping.csv\")\n",
    "data.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "with open('./MLModels/xgboost_best_model.pkl', 'rb') as f:\n",
    "   xgb_model = pickle.load(f)\n",
    "with open('./MLModels/linear_svc_final_model.pkl', 'rb') as f:\n",
    "   svc_model = pickle.load(f)\n",
    "    \n",
    "# Embed Data\n",
    "embedder = Embedders_Five(data[\"User Story\"])\n",
    "bert_samples = embedder.getBERTEmbeddings()\n",
    "glove_samples = embedder.getGloVEEmbedding()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReFAIR prediction and export the new excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Story</th>\n",
       "      <th>Oracle Domain</th>\n",
       "      <th>Oracle Tasks &amp; Sensitive Features</th>\n",
       "      <th>Oracle Unique Sensitive Features</th>\n",
       "      <th>ReFAIR Domain</th>\n",
       "      <th>ReFAIR Tasks &amp; Sensitive Features</th>\n",
       "      <th>ReFAIR Unique Sensitive Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A bioinformatics company is using statistical ...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{'classification': [], 'graph mining': []}</td>\n",
       "      <td>{}</td>\n",
       "      <td>biology</td>\n",
       "      <td>{}</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A bioinformatics company is using text generat...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{'graph mining': []}</td>\n",
       "      <td>{}</td>\n",
       "      <td>biology</td>\n",
       "      <td>{}</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A bioinformatics company is using visual quest...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{'classification': []}</td>\n",
       "      <td>{}</td>\n",
       "      <td>biology</td>\n",
       "      <td>{'classification': []}</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A bioinformatics researcher is developing a sy...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{'graph mining': []}</td>\n",
       "      <td>{}</td>\n",
       "      <td>biology</td>\n",
       "      <td>{'graph mining': []}</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A bioinformatics researcher is interested in i...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>biology</td>\n",
       "      <td>{}</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>A literary researcher is using machine learnin...</td>\n",
       "      <td>Literature</td>\n",
       "      <td>{'clustering': ['author', 'gender', 'textual r...</td>\n",
       "      <td>{'author', 'gender', 'textual references to pe...</td>\n",
       "      <td>literature</td>\n",
       "      <td>{'clustering': ['textual references to people ...</td>\n",
       "      <td>{'textual references to people and their demog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A literary scholar wants to develop a bidirect...</td>\n",
       "      <td>Literature</td>\n",
       "      <td>{'ranking': ['gender']}</td>\n",
       "      <td>{'gender'}</td>\n",
       "      <td>literature</td>\n",
       "      <td>{}</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>A literary scholar wants to use automated patt...</td>\n",
       "      <td>Literature</td>\n",
       "      <td>{'ranking': ['gender'], 'clustering': ['author...</td>\n",
       "      <td>{'author', 'gender', 'textual references to pe...</td>\n",
       "      <td>literature</td>\n",
       "      <td>{'clustering': ['textual references to people ...</td>\n",
       "      <td>{'textual references to people and their demog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>A literary scholar wants to use bootstrap aggr...</td>\n",
       "      <td>Literature</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>literature</td>\n",
       "      <td>{}</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>A literary scholar wants to use kohonen neural...</td>\n",
       "      <td>Literature</td>\n",
       "      <td>{'clustering': ['author', 'gender', 'textual r...</td>\n",
       "      <td>{'author', 'gender', 'textual references to pe...</td>\n",
       "      <td>literature</td>\n",
       "      <td>{'clustering': ['textual references to people ...</td>\n",
       "      <td>{'textual references to people and their demog...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           User Story Oracle Domain  \\\n",
       "0   A bioinformatics company is using statistical ...       Biology   \n",
       "1   A bioinformatics company is using text generat...       Biology   \n",
       "2   A bioinformatics company is using visual quest...       Biology   \n",
       "3   A bioinformatics researcher is developing a sy...       Biology   \n",
       "4   A bioinformatics researcher is interested in i...       Biology   \n",
       "..                                                ...           ...   \n",
       "95  A literary researcher is using machine learnin...    Literature   \n",
       "96  A literary scholar wants to develop a bidirect...    Literature   \n",
       "97  A literary scholar wants to use automated patt...    Literature   \n",
       "98  A literary scholar wants to use bootstrap aggr...    Literature   \n",
       "99  A literary scholar wants to use kohonen neural...    Literature   \n",
       "\n",
       "                    Oracle Tasks & Sensitive Features  \\\n",
       "0          {'classification': [], 'graph mining': []}   \n",
       "1                                {'graph mining': []}   \n",
       "2                              {'classification': []}   \n",
       "3                                {'graph mining': []}   \n",
       "4                                                  {}   \n",
       "..                                                ...   \n",
       "95  {'clustering': ['author', 'gender', 'textual r...   \n",
       "96                            {'ranking': ['gender']}   \n",
       "97  {'ranking': ['gender'], 'clustering': ['author...   \n",
       "98                                                 {}   \n",
       "99  {'clustering': ['author', 'gender', 'textual r...   \n",
       "\n",
       "                     Oracle Unique Sensitive Features ReFAIR Domain  \\\n",
       "0                                                  {}       biology   \n",
       "1                                                  {}       biology   \n",
       "2                                                  {}       biology   \n",
       "3                                                  {}       biology   \n",
       "4                                                  {}       biology   \n",
       "..                                                ...           ...   \n",
       "95  {'author', 'gender', 'textual references to pe...    literature   \n",
       "96                                         {'gender'}    literature   \n",
       "97  {'author', 'gender', 'textual references to pe...    literature   \n",
       "98                                                 {}    literature   \n",
       "99  {'author', 'gender', 'textual references to pe...    literature   \n",
       "\n",
       "                    ReFAIR Tasks & Sensitive Features  \\\n",
       "0                                                  {}   \n",
       "1                                                  {}   \n",
       "2                              {'classification': []}   \n",
       "3                                {'graph mining': []}   \n",
       "4                                                  {}   \n",
       "..                                                ...   \n",
       "95  {'clustering': ['textual references to people ...   \n",
       "96                                                 {}   \n",
       "97  {'clustering': ['textual references to people ...   \n",
       "98                                                 {}   \n",
       "99  {'clustering': ['textual references to people ...   \n",
       "\n",
       "                     ReFAIR Unique Sensitive Features  \n",
       "0                                               set()  \n",
       "1                                               set()  \n",
       "2                                               set()  \n",
       "3                                               set()  \n",
       "4                                               set()  \n",
       "..                                                ...  \n",
       "95  {'textual references to people and their demog...  \n",
       "96                                              set()  \n",
       "97  {'textual references to people and their demog...  \n",
       "98                                              set()  \n",
       "99  {'textual references to people and their demog...  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the exact same order as during training\n",
    "model_domains = np.array(['Biology', 'Plant Science', 'Economics', 'Finance & Marketing',\n",
    "       'Information Systems', 'News', 'Law', 'Political Science', 'Library',\n",
    "       'Linguistics', 'Literature', 'Cardiology', 'Dermatology', 'Endocrinology',\n",
    "       'Health', 'Medicine', 'Nephrology', 'Pediatrics', 'Pharmacology', 'Psychology',\n",
    "       'Radiology', 'Psycology', 'Demography', 'Education', 'Social Media',\n",
    "       'Social Networks', 'Social Work', 'Sociology', 'Transportation',\n",
    "       'Urban Studies', 'Demograpy', 'Social networks', 'Social work', 'Movies',\n",
    "       'Music', 'Sport', 'Computer Networks', 'Computer Vision'])\n",
    "\n",
    "def standardize_domain(predicted_idx):\n",
    "    \"\"\"Maps the predicted index to standardized domain name\"\"\"\n",
    "    predicted_domain = model_domains[predicted_idx].lower()\n",
    "    \n",
    "    # Fix known typos\n",
    "    if predicted_domain == 'psycology':\n",
    "        return 'psychology'\n",
    "    elif predicted_domain == 'demograpy':\n",
    "        return 'demography'\n",
    "    elif predicted_domain == 'social networks':  # Handle duplicate with different case\n",
    "        return 'social networks'\n",
    "    elif predicted_domain == 'social work':      # Handle duplicate with different case\n",
    "        return 'social work'\n",
    "    \n",
    "    return predicted_domain\n",
    "\n",
    "# Process predictions\n",
    "refair_predictions = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    user_story = data.iloc[i]['User Story']\n",
    "    \n",
    "    # Get standardized domain prediction\n",
    "    predicted_domain = standardize_domain(domain_predictions[i])\n",
    "    \n",
    "    # Rest of your prediction code...\n",
    "    task_vector = task_predictions[i].toarray()[0]\n",
    "    predicted_tasks = [mltasks_classes[j] for j, val in enumerate(task_vector) if val == 1]\n",
    "    \n",
    "    # Format tasks dictionary\n",
    "    tasks_features_dict = {}\n",
    "    if predicted_tasks:\n",
    "        for task in predicted_tasks:\n",
    "            # Check if task is valid for predicted domain\n",
    "            valid_task = domain_task_mapping[\n",
    "                (domain_task_mapping['Domain'].str.lower() == predicted_domain.lower()) & \n",
    "                (domain_task_mapping['Task'].str.lower() == task.lower())\n",
    "            ].shape[0] > 0\n",
    "            \n",
    "            if valid_task:\n",
    "                tasks_features_dict[task] = []\n",
    "    \n",
    "    # Get sensitive features\n",
    "    if tasks_features_dict:\n",
    "        refair_tasks_features = mapper.get_sensitive_features(list(tasks_features_dict.keys()), predicted_domain)\n",
    "        refair_unique_features = mapper.get_unique_sensitive_features(list(tasks_features_dict.keys()), predicted_domain)\n",
    "    else:\n",
    "        refair_tasks_features = {}\n",
    "        refair_unique_features = set()\n",
    "    \n",
    "    refair_row = {\n",
    "        'User Story': user_story,\n",
    "        'ReFAIR Domain': predicted_domain,\n",
    "        'ReFAIR Tasks & Sensitive Features': str(refair_tasks_features),\n",
    "        'ReFAIR Unique Sensitive Features': str(refair_unique_features)\n",
    "    }\n",
    "    refair_predictions.append(refair_row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "refair_df = pd.DataFrame(refair_predictions)\n",
    "final_df = pd.concat([data, refair_df[['ReFAIR Domain', 'ReFAIR Tasks & Sensitive Features', 'ReFAIR Unique Sensitive Features']]], axis=1)\n",
    "\n",
    "final_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved to /home/ahmedyra/projects/def-hinat/ahmedyra/EECS6448Project/CanWeTrustReFAIR/Dataset/SensitiveFeaturesValidation_Data/overall-oracle-prediciton.csv\n"
     ]
    }
   ],
   "source": [
    "for col in final_df.select_dtypes(include='object').columns:\n",
    "    final_df[col] = final_df[col].str.lower()\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = '/home/ahmedyra/projects/def-hinat/ahmedyra/EECS6448Project/CanWeTrustReFAIR/Dataset/SensitiveFeaturesValidation_Data/overall-oracle-prediciton.csv'\n",
    "\n",
    "# Export the DataFrame to CSV\n",
    "final_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"CSV file saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
